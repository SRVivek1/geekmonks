<!DOCTYPE html>
<html lang="en">

<!-- HTML Head-->
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css" rel="stylesheet">
    <link rel="stylesheet" href="/geekmonks/assets/css/styles.css">
    <link type="application/atom+xml" rel="alternate" href="https://srvivek1.github.io/geekmonks/feed.xml" title="Geekmonks Blog" />
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Ollama - Run LLMs Locally | Geekmonks Blog</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Ollama - Run LLMs Locally" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Let’s dive into setting up a robust local development environment suitable for begineer/intermediate LLM engineering tasks. We focus on modern, efficient tools like Ollama for running models locally and uv as an ultrafast python package manager." />
<meta property="og:description" content="Let’s dive into setting up a robust local development environment suitable for begineer/intermediate LLM engineering tasks. We focus on modern, efficient tools like Ollama for running models locally and uv as an ultrafast python package manager." />
<link rel="canonical" href="https://srvivek1.github.io/geekmonks/ai-ml/llm-engineering/ollama-local-setup.html" />
<meta property="og:url" content="https://srvivek1.github.io/geekmonks/ai-ml/llm-engineering/ollama-local-setup.html" />
<meta property="og:site_name" content="Geekmonks Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-10-25T00:00:00+05:30" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Ollama - Run LLMs Locally" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-10-25T00:00:00+05:30","datePublished":"2025-10-25T00:00:00+05:30","description":"Let’s dive into setting up a robust local development environment suitable for begineer/intermediate LLM engineering tasks. We focus on modern, efficient tools like Ollama for running models locally and uv as an ultrafast python package manager.","headline":"Ollama - Run LLMs Locally","mainEntityOfPage":{"@type":"WebPage","@id":"https://srvivek1.github.io/geekmonks/ai-ml/llm-engineering/ollama-local-setup.html"},"url":"https://srvivek1.github.io/geekmonks/ai-ml/llm-engineering/ollama-local-setup.html"}</script>
<!-- End Jekyll SEO tag -->

    <!-- Google Analytics -->
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-48XEHMVSEC"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-48XEHMVSEC');
</script>
</head>

<body class="container-fluid p-0 m-0">
    <header>
        <nav class="navbar navbar-expand-md navbar-light bg-light text-white content-shadow">
  <div class="container d-flex justify-content-between">
    <a href="/geekmonks/" class="navbar-brand d-flex">
      <img class="d-inline-block align-top" src="/geekmonks/assets/icons/laptop.svg" height="40"
        width="34" />
      <h1 class="fs-3 px-2 fw-bold">Geekmonks</h1>
      <img class="d-inline-block align-top" src="/geekmonks/assets/icons/tux.svg" height="40"
        width="34" />
    </a>
    <!-- Toggle Button to expand and collapse the nav links -->
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNavBanner"
      aria-controls="navbarNavBanner" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbarNavBanner" class="collapse navbar-collapse">
      <ul class="navbar-nav ms-auto">
        
        <li class="nav-item active">
          <a href="/geekmonks/" class="nav-link fw-bold active"
            >Home</a>
        </li>
        
        <li class="nav-item active">
          <a href="/geekmonks/about" class="nav-link fw-bold active"
            >About</a>
        </li>
        
        <li class="nav-item active">
          <a href="https://github.com/SRVivek1/" class="nav-link fw-bold active"
            target="_blank" rel="noopener noreferrer">Github</a>
        </li>
        
        <li class="nav-item active">
          <a href="https://www.linkedin.com/in/srvivek1/" class="nav-link fw-bold active"
            target="_blank" rel="noopener noreferrer">LinkedIn</a>
        </li>
        
      </ul>
    </div>
  </div>
</nav>
        <nav class="navbar navbar-expand-xl navbar-custom p-0">
  <div class="container-fluid">

    <!-- Toggler for small screens -->
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <!-- Collapsible nav items -->
    <div class="collapse navbar-collapse" id="navbarNav">
      <ul class="navbar-nav mx-auto">
        
          
          
            
              <!-- Simple nav item -->
              <li class="nav-item">
                <a class="nav-link fw-bold text-white" href="/geekmonks/ai-ml/llm-engineering/"
                  >AI-ML</a>
              </li>
            
          
            
              <!-- Simple nav item -->
              <li class="nav-item">
                <a class="nav-link fw-bold text-white" href="/geekmonks/cloud/aws/"
                  >AWS</a>
              </li>
            
          
            
              <!-- Simple nav item -->
              <li class="nav-item">
                <a class="nav-link fw-bold text-white" href="/geekmonks/spring/spring-boot/"
                  >Spring Boot</a>
              </li>
            
          
        
      </ul>
    </div>
  </div>
</nav>
    </header>

    <div class="container-fluid">
        <div class="row min-vh-100">
            <!-- Left Sidebar -->
            <nav id="course-sidebar" class="col-md-3 col-lg-2 d-md-block bg-light sidebar collapse">
                <div class="position-sticky pt-3">
                    <ul class="nav flex-column course-topics-list">
                        
                        
                        
                        
                        
                        
                        <a class="nav-link course-link active"
                            href="/geekmonks/ai-ml/llm-engineering" data-course="ai-ml/llm-engineering">
                            AI/ML
                        </a>
                        <!-- Debug - Submenu links are broken -->
                        
                        <ul class="nav flex-column ms-3 subtopics" id="topics-ai-ml-llm-engineering">
                            
                            <li class="nav-item" style="text-align: left">
                                <a class="nav-link topic-link small " href="introduction-to-llms.html"
                                    data-topic="introduction-to-llms.html"
                                    data-course-path="/ai-ml/llm-engineering">
                                    LLM Fundamentals
                                </a>
                            </li>
                            
                            <li class="nav-item" style="text-align: left">
                                <a class="nav-link topic-link small active" href="ollama-local-setup.html"
                                    data-topic="ollama-local-setup.html"
                                    data-course-path="/ai-ml/llm-engineering">
                                    Lab - Ollama Local LLMs
                                </a>
                            </li>
                            
                            <li class="nav-item" style="text-align: left">
                                <a class="nav-link topic-link small " href="b1-t1-hello-world.html"
                                    data-topic="b1-t1-hello-world.html"
                                    data-course-path="/ai-ml/llm-engineering">
                                    Lab - Python App Integration
                                </a>
                            </li>
                            
                            <li class="nav-item" style="text-align: left">
                                <a class="nav-link topic-link small " href="ollama-ai-assistant.html"
                                    data-topic="ollama-ai-assistant.html"
                                    data-course-path="/ai-ml/llm-engineering">
                                    Lab - Local AI Assistant
                                </a>
                            </li>
                            
                            <li class="nav-item" style="text-align: left">
                                <a class="nav-link topic-link small " href="frontier-models-capabilities-operational-risks.html"
                                    data-topic="frontier-models-capabilities-operational-risks.html"
                                    data-course-path="/ai-ml/llm-engineering">
                                    Architecture & capabilities
                                </a>
                            </li>
                            
                            <li class="nav-item" style="text-align: left">
                                <a class="nav-link topic-link small " href="foundaion-and-evolution.html"
                                    data-topic="foundaion-and-evolution.html"
                                    data-course-path="/ai-ml/llm-engineering">
                                    Foundation and Evaluation
                                </a>
                            </li>
                            
                            <li class="nav-item" style="text-align: left">
                                <a class="nav-link topic-link small " href="advance-concepts-toknization-and-scaling.html"
                                    data-topic="advance-concepts-toknization-and-scaling.html"
                                    data-course-path="/ai-ml/llm-engineering">
                                    Advanced - Tokens, Scaling
                                </a>
                            </li>
                            
                            <li class="nav-item" style="text-align: left">
                                <a class="nav-link topic-link small " href="scaling-reasoning-inference.html"
                                    data-topic="scaling-reasoning-inference.html"
                                    data-course-path="/ai-ml/llm-engineering">
                                    Advanced - Reasoning, Interference
                                </a>
                            </li>
                            
                            <li class="nav-item" style="text-align: left">
                                <a class="nav-link topic-link small " href="payload-tokenization.html"
                                    data-topic="payload-tokenization.html"
                                    data-course-path="/ai-ml/llm-engineering">
                                    Advanced - Payload Tokenization
                                </a>
                            </li>
                            
                            <li class="nav-item" style="text-align: left">
                                <a class="nav-link topic-link small " href="advance-conectps-models-deployments.html"
                                    data-topic="advance-conectps-models-deployments.html"
                                    data-course-path="/ai-ml/llm-engineering">
                                    Advanced Concepts & Deployment
                                </a>
                            </li>
                            
                            <li class="nav-item" style="text-align: left">
                                <a class="nav-link topic-link small " href="advance-prompt-caching.html"
                                    data-topic="advance-prompt-caching.html"
                                    data-course-path="/ai-ml/llm-engineering">
                                    Advanced - Prompt Caching
                                </a>
                            </li>
                            
                        </ul>
                        
                        
                        
                        
                    </ul>
                </div>
            </nav>

            <!-- Right Content Panel -->
            <main class="col-md-9 ms-sm-auto col-lg-10 px-md-4">
                <!-- Mobile: toggle sidebar button -->
                <button id="sidebarToggle" class="btn btn-sm btn-outline-secondary d-md-none mb-2"
                    aria-label="Toggle topics" aria-controls="course-sidebar">
                    ☰ Topics
                </button>



                <div id="topic-content">
                    <div id="pageTitle">
                        <h1>Ollama - Run LLMs Locally</h1>
                        <p>
                            Updated on: 25 Oct 2025

                            
                            
                            - <a href="/geekmonks/authors/srvivek/">Vivek Singh</a>
                            
                        </p>

                        <hr class="stylish-hr"/>
                    </div>

                    <p>Let’s dive into setting up a robust local development environment suitable for begineer/intermediate LLM engineering tasks. We focus on modern, efficient tools like <strong>Ollama</strong> for running models locally and <strong>uv</strong> as an ultrafast python package manager.</p>

<h2 id="table-of-contents">Table of Contents</h2>

<ul>
  <li><a href="#table-of-contents">Table of Contents</a></li>
  <li><a href="#1-overview">1. Overview</a></li>
  <li><a href="#2-ollama-installation-for-local-models">2. Ollama Installation for Local Models</a>
    <ul>
      <li><a href="#installation-instructions">Installation Instructions</a></li>
      <li><a href="#post-installation-verification">Post-Installation Verification</a></li>
      <li><a href="#api-verification">API verification</a></li>
    </ul>
  </li>
  <li><a href="#3-integrated-development-environment-ide-setup">3. Integrated Development Environment (IDE) Setup</a>
    <ul>
      <li><a href="#ide-choice-and-update">IDE Choice and Update</a></li>
      <li><a href="#required-extensions-vs-codecursor">Required Extensions (VS Code/Cursor)</a></li>
    </ul>
  </li>
  <li><a href="#4-the-uv-python-package-and-project-manager">4. The <code class="language-plaintext highlighter-rouge">uv</code> Python Package and Project Manager</a>
    <ul>
      <li><a href="#installation-of-uv">Installation of <code class="language-plaintext highlighter-rouge">uv</code></a></li>
      <li><a href="#verification-and-maintenance">Verification and Maintenance</a></li>
    </ul>
  </li>
  <li><a href="#5-building-the-python-project">5. Building the Python Project</a>
    <ul>
      <li><a href="#project-synchronization">Project Synchronization</a></li>
    </ul>
  </li>
  <li><a href="#6-troubleshooting-missing-jupyter-kernel">6. Troubleshooting: Missing Jupyter Kernel</a>
    <ul>
      <li><a href="#manual-kernel-registration-steps">Manual Kernel Registration Steps</a></li>
    </ul>
  </li>
  <li><a href="#7-conclusion-and-key-takeaways">7. Conclusion and Key Takeaways</a>
    <ul>
      <li><a href="#key-takeaways">Key Takeaways</a></li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="1-overview">1. Overview</h2>

<p>A stable, performant local environment is critical for iterating quickly on LLM projects. This setup utilizes <strong>Ollama</strong> for running open-source models (like Llama 3 or Mistral) directly on your machine and <strong>VS Code/Cursor</strong> as the primary development platform, enhanced by the Rust-based package manager <strong>uv</strong> for superior speed and reliability.</p>

<hr />

<h2 id="2-ollama-installation-for-local-models">2. Ollama Installation for Local Models</h2>

<p><strong>Ollama</strong> simplifies running and managing large language models locally. It provides a single executable for downloading, configuring, and serving models via an API.</p>

<h3 id="installation-instructions">Installation Instructions</h3>

<p>Follow the platform-specific instructions below. The primary goal is to install the <code class="language-plaintext highlighter-rouge">ollama</code> command-line tool.</p>

<ul>
  <li><strong>Linux (Recommended via Script):</strong>
The installation script automatically handles setting up the service.
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-fsSL</span> https://ollama.com/install.sh | sh
</code></pre></div>    </div>
  </li>
  <li><strong>macOS and Windows (GUI/Installer):</strong>
For these platforms, use the dedicated installers which often include a graphical user interface (GUI) and better system integration.
    <ul>
      <li><strong>macOS Installer:</strong> <a href="https://ollama.com/download/mac">Download Ollama for Mac</a></li>
      <li><strong>Windows Installer:</strong> <a href="https://ollama.com/download/windows">Download Ollama for Windows</a></li>
    </ul>
  </li>
</ul>

<h3 id="post-installation-verification">Post-Installation Verification</h3>

<p>Once installed, you can pull and run your first model, for example, the tiny <code class="language-plaintext highlighter-rouge">llama3:8b</code> model:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ollama run llama3:8b
</code></pre></div></div>

<p>This command downloads the model if it’s not present and starts an interactive chat session via <strong><em>terminal/CMD</em></strong>.</p>

<h3 id="api-verification">API verification</h3>
<p>You can run below curl command from terminal or use any availale HTTP Client tool to mimic below request and send the message to Ollama API.</p>
<ul>
  <li>If the provided LLM model is available locally, <code class="language-plaintext highlighter-rouge">Ollama</code> with run the model and servs the request. While running model in this way Ollama sets a default max-life of 4 Minutes for that model post which the model will be terminated automatically.</li>
  <li><code class="language-plaintext highlighter-rouge">Stream</code> property is be default set to <code class="language-plaintext highlighter-rouge">True</code>. This enables the Ollama to keep sending the data to terminal as the model thinks and generates the response but if set to <code class="language-plaintext highlighter-rouge">False</code> it will process the request and send only the final response back to requestor.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl http://localhost:11434/v1/chat/completions <span class="se">\</span>
  <span class="nt">-H</span> <span class="s2">"Content-Type: application/json"</span> <span class="se">\</span>
  <span class="nt">-d</span> <span class="s1">'{
    "model": "gemma3:1b",
    "messages": [{"role": "user", "content": "Tell me a fun fact"}],
    "stream": false
  }'</span>
</code></pre></div></div>
<hr />

<h2 id="3-integrated-development-environment-ide-setup">3. Integrated Development Environment (IDE) Setup</h2>

<p>For LLM engineering, an IDE with robust support for <strong>Python</strong>, <strong>Jupyter Notebooks</strong>, and <strong>AI-assisted coding assistand</strong> is essential.</p>

<h3 id="ide-choice-and-update">IDE Choice and Update</h3>

<ul>
  <li><strong>Recommended IDEs:</strong> <strong>Visual Studio Code (VS Code)</strong> or <strong>Cursor</strong>.
    <ul>
      <li>Cursor is often preferred for its built-in, deep AI coding agent capabilities, but VS Code is the industry standard.</li>
    </ul>
  </li>
  <li><strong>Action:</strong> Ensure you are running the <strong>latest stable version</strong> of your chosen IDE for the best feature and security support.</li>
</ul>

<h3 id="required-extensions-vs-codecursor">Required Extensions (VS Code/Cursor)</h3>

<p>Two core extensions from Microsoft are necessary to support a modern Python and data science workflow:</p>

<ol>
  <li><strong>Python Extension (Microsoft):</strong> Provides rich support for Python development, including IntelliSense, debugging, code navigation, and environment management.
    <ul>
      <li><em>Tip:</em> Search the VS Code Marketplace for <code class="language-plaintext highlighter-rouge">ms-python.python</code>.</li>
    </ul>
  </li>
  <li><strong>Jupyter Extension (Microsoft):</strong> Enables full functionality for working with <code class="language-plaintext highlighter-rouge">.ipynb</code> files, including variable inspection, cell execution, and kernel management, crucial for research and prototyping in LLM development.
    <ul>
      <li><em>Tip:</em> Search the VS Code Marketplace for <code class="language-plaintext highlighter-rouge">ms-toolsai.jupyter</code>.</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="4-the-uv-python-package-and-project-manager">4. The <code class="language-plaintext highlighter-rouge">uv</code> Python Package and Project Manager</h2>

<p><strong>uv</strong> is an extremely fast and reliable Python package and project manager written in Rust, designed to be a drop-in replacement for common <code class="language-plaintext highlighter-rouge">pip</code>, <code class="language-plaintext highlighter-rouge">pip-tools</code>, and <code class="language-plaintext highlighter-rouge">virtualenv</code> workflows. It significantly accelerates dependency resolution and virtual environment creation.</p>

<h3 id="installation-of-uv">Installation of <code class="language-plaintext highlighter-rouge">uv</code></h3>

<p>The recommended method is via the installation script:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install uv via the official script</span>
curl <span class="nt">-LsSf</span> https://astral.sh/uv/install.sh | sh
</code></pre></div></div>

<h3 id="verification-and-maintenance">Verification and Maintenance</h3>

<p>After installation, verify the version and know how to keep it updated:</p>

<ul>
  <li><strong>Verify Installation:</strong>
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv <span class="nt">--version</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Update <code class="language-plaintext highlighter-rouge">uv</code>:</strong>
Keep your <code class="language-plaintext highlighter-rouge">uv</code> installation current to benefit from the latest speed improvements and features.
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv self update
</code></pre></div>    </div>
  </li>
</ul>

<hr />

<h2 id="5-building-the-python-project">5. Building the Python Project</h2>

<p>Assuming you have a standard Python project structure and a <code class="language-plaintext highlighter-rouge">requirements.txt</code> or <code class="language-plaintext highlighter-rouge">pyproject.toml</code> file defining your dependencies, <code class="language-plaintext highlighter-rouge">uv</code> simplifies the process of creating a virtual environment (<code class="language-plaintext highlighter-rouge">venv</code>) and installing dependencies.</p>

<h3 id="project-synchronization">Project Synchronization</h3>

<p>The <code class="language-plaintext highlighter-rouge">uv sync</code> command is the equivalent of <code class="language-plaintext highlighter-rouge">pip install -r requirements.txt</code> or <code class="language-plaintext highlighter-rouge">pip-sync</code>, but much faster. It creates the virtual environment and installs/updates all dependencies defined in your project files.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Command to build the virtual environment and install dependencies</span>
uv <span class="nb">sync</span>
</code></pre></div></div>

<ul>
  <li><strong>Note:</strong> If a <code class="language-plaintext highlighter-rouge">.venv</code> directory does not exist, <code class="language-plaintext highlighter-rouge">uv</code> will automatically create it based on your project files and then synchronize the required packages inside it.</li>
</ul>

<hr />

<h2 id="6-troubleshooting-missing-jupyter-kernel">6. Troubleshooting: Missing Jupyter Kernel</h2>

<p>A common issue in IDEs like VS Code is the inability to detect the dedicated kernel within a new virtual environment when opening a Jupyter Notebook. This prevents running code cells with the project’s specific dependencies.</p>

<p>To resolve this, manually register the virtual environment as a usable Jupyter kernel.</p>

<h3 id="manual-kernel-registration-steps">Manual Kernel Registration Steps</h3>

<ol>
  <li>
    <p><strong>Activate the Virtual Environment:</strong> Navigate to your project root and activate the environment to ensure <code class="language-plaintext highlighter-rouge">uv</code> and <code class="language-plaintext highlighter-rouge">python</code> are linked to the correct binaries.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">source</span> .venv/bin/activate
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>Run the <code class="language-plaintext highlighter-rouge">ipykernel</code> Install Command:</strong> Use <code class="language-plaintext highlighter-rouge">uv run</code> to execute the Python interpreter within the virtual environment, calling the <code class="language-plaintext highlighter-rouge">ipykernel</code> module to register the environment.</p>

    <ul>
      <li><code class="language-plaintext highlighter-rouge">--user</code>: Installs the kernel for the current user.</li>
      <li><code class="language-plaintext highlighter-rouge">--name</code>: The internal name for the kernel (e.g., <code class="language-plaintext highlighter-rouge">llm-eng</code>).</li>
      <li><code class="language-plaintext highlighter-rouge">--display-name</code>: The name visible in the IDE’s kernel selector.</li>
    </ul>

    <!-- end list -->

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv run python <span class="nt">-m</span> ipykernel <span class="nb">install</span> <span class="nt">--user</span> <span class="nt">--name</span><span class="o">=</span>llm-eng <span class="nt">--display-name</span><span class="o">=</span><span class="s2">"LLM Eng (.venv)"</span>
</code></pre></div>    </div>
  </li>
</ol>

<p>After running this, the kernel named <strong>“LLM Eng (.venv)”</strong> will appear in the kernel selector of your Jupyter Notebook in VS-Code, allowing you to execute the code blocks in <code class="language-plaintext highlighter-rouge">.ipynb</code> files directly from IDE with your project’s installed packages.</p>

<hr />

<h2 id="7-conclusion-and-key-takeaways">7. Conclusion and Key Takeaways</h2>

<p>The modern LLM engineering workflow prioritizes speed and local experimentation. By leveraging <strong>Ollama</strong> for model serving and <strong>uv</strong> for dependency management, developers can achieve rapid iteration cycles.</p>

<h3 id="key-takeaways">Key Takeaways</h3>

<ul>
  <li><strong>Ollama:</strong> Enables simple, powerful execution of open-source LLMs locally, reducing cloud costs during development.</li>
  <li><strong>uv:</strong> Drastically cuts down the time spent on dependency resolution and environment setup, freeing up time for actual engineering.</li>
  <li><strong>IDE Setup:</strong> Installing the <strong>Python</strong> and <strong>Jupyter</strong> extensions is non-negotiable for a professional LLM development experience.</li>
  <li><strong>Kernel Registration:</strong> Knowing how to manually register a kernel is an essential troubleshooting skill for any developer working with isolated Python environments and Jupyter notebooks.</li>
</ul>

<p>Would you like to move on to documenting the LLM-specific techniques, such as <strong>Prompt Engineering</strong> and <strong>RAG vs. Fine-Tuning</strong>, that were mentioned in the session overview?</p>

<!-- Adding a gray border in bottom of page. -->
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code></code></pre></div></div>
 <!-- Injects index.md content here -->
                </div>
            </main>
        </div>
    </div>

    <!-- External scripts -->
    <!-- bootstrap -->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js" defer></script>

<!-- Mermaid JS: render diagrams client-side. We convert fenced code blocks
		 with class `language-mermaid` into <div class="mermaid"> so existing
		 markdown files don't need editing. -->
<script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
<!-- Include internal JavaScript -->
 <script src="/geekmonks/assets/js/mermaid-diagram-render.js"></script>


 <!-- Submenu toggle nav on small screen-->
<script src="/geekmonks/assets/js/submenu-togle.js"></script>


<script src="/geekmonks/assets/js/unique-url-layout.js"></script>

</body>

</html>