<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta http-equiv="refresh" content="10; url=https://geekmonks.com/"><meta name="keywords" content="geekmonks, geek, tech, geekmonks tech, free tutorial, free online tutorial"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="apple-mobile-web-app-title" content="Geekmonks Tech"><meta name="mobile-web-app-capable" content="yes"><link rel="icon" type="image/png" href="/geekmonks/assets/favicon/favicon-96x96.png" sizes="96x96"/><link rel="icon" type="image/svg+xml" href="/geekmonks/assets/favicon/favicon.svg"/><link rel="shortcut icon" href="/geekmonks/assets/favicon/favicon.ico"/><link rel="apple-touch-icon" sizes="180x180" href="/geekmonks/assets/favicon/apple-touch-icon.png"/><link rel="manifest" href="/geekmonks/assets/favicon/site.webmanifest"/><link type="application/atom+xml" rel="alternate" href="https://srvivek1.github.io/geekmonks/feed.xml" title="Geekmonks Tech"/><title>Ollama - Run LLMs Locally | Geekmonks Tech</title><meta name="generator" content="Jekyll v3.10.0"/><meta property="og:title" content="Ollama - Run LLMs Locally"/><meta property="og:locale" content="en_US"/><meta name="description" content="Geekmonks Tech - AI Tutorial, ML Tutorial, Free AI ML tutorial, Free Online Tutorials, geekmonks provides tutorials and interview questions for AI, ML, Introduction to LLM Engineering, ollama, local setup, coding assistnt, continue extension, pythong uv, installation, jupyter notes, jupyter kernel, vscode etc. for beginners and professionals."/><meta property="og:description" content="Geekmonks Tech - AI Tutorial, ML Tutorial, Free AI ML tutorial, Free Online Tutorials, geekmonks provides tutorials and interview questions for AI, ML, Introduction to LLM Engineering, ollama, local setup, coding assistnt, continue extension, pythong uv, installation, jupyter notes, jupyter kernel, vscode etc. for beginners and professionals."/><link rel="canonical" href="https://srvivek1.github.io/geekmonks/ai-ml/llm-engineering/ollama-local-setup.html"/><meta property="og:url" content="https://srvivek1.github.io/geekmonks/ai-ml/llm-engineering/ollama-local-setup.html"/><meta property="og:site_name" content="Geekmonks Tech"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2025-10-07T18:00:00+05:30"/><meta name="twitter:card" content="summary"/><meta property="twitter:title" content="Ollama - Run LLMs Locally"/><meta name="twitter:site" content="@srvivek_"/><script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-10-07T18:00:00+05:30","datePublished":"2025-10-07T18:00:00+05:30","description":"Geekmonks Tech - AI Tutorial, ML Tutorial, Free AI ML tutorial, Free Online Tutorials, geekmonks provides tutorials and interview questions for AI, ML, Introduction to LLM Engineering, ollama, local setup, coding assistnt, continue extension, pythong uv, installation, jupyter notes, jupyter kernel, vscode etc. for beginners and professionals.","headline":"Ollama - Run LLMs Locally","mainEntityOfPage":{"@type":"WebPage","@id":"https://srvivek1.github.io/geekmonks/ai-ml/llm-engineering/ollama-local-setup.html"},"url":"https://srvivek1.github.io/geekmonks/ai-ml/llm-engineering/ollama-local-setup.html"}</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-48XEHMVSEC"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date);gtag("config","G-48XEHMVSEC");</script><link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet"><link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css" rel="stylesheet"><link rel="stylesheet" href="/geekmonks/assets/css/styles.css?v=0.1.0"></head><body class="container-fluid p-0 m-0"><p class="newPortalRedirect">You will be redirect to our new portal, Happy Learning. If you are not redirected automatically, please click <a href="https://geekmonks.com/">here</a>.</p><header><nav class="navbar navbar-expand-md navbar-light bg-light text-white content-shadow"><div class="container d-flex justify-content-between"><a href="/geekmonks/" class="navbar-brand d-flex"><img class="d-inline-block align-top" src="/geekmonks/assets/icons/laptop.svg" height="40" width="34" alt="banner-1 laptop icon"/><h1 class="fs-3 px-2 fw-bold">Geekmonks</h1><img class="d-inline-block align-top" src="/geekmonks/assets/icons/tux.svg" height="40" width="34" alt="banner-2 penguine icon"/></a><button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNavBanner" aria-controls="navbarNavBanner" aria-expanded="false" aria-label="Toggle Navigation"><span class="navbar-toggler-icon"></span></button><div id="navbarNavBanner" class="collapse navbar-collapse"><ul class="navbar-nav ms-auto"><li class="nav-item active"><a href="/geekmonks/" class="nav-link fw-bold active">Home</a></li><li class="nav-item active"><a href="/geekmonks/about" class="nav-link fw-bold active">About</a></li><li class="nav-item active"><a href="https://github.com/SRVivek1/" class="nav-link fw-bold active" target="_blank" rel="noopener noreferrer">Github</a></li><li class="nav-item active"><a href="https://www.linkedin.com/in/srvivek1/" class="nav-link fw-bold active" target="_blank" rel="noopener noreferrer">LinkedIn</a></li></ul></div></div></nav><nav class="navbar navbar-expand-xl navbar-custom p-0"><div class="container-fluid"><button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNav"><ul class="navbar-nav mx-auto"><li class="nav-item"><a class="nav-link fw-bold text-white" href="/geekmonks/ai-ml/llm-engineering/">AI-ML</a></li><li class="nav-item"><a class="nav-link fw-bold text-white" href="/geekmonks/cloud/aws/">AWS</a></li><li class="nav-item"><a class="nav-link fw-bold text-white" href="/geekmonks/spring/spring-boot/">Spring Boot</a></li></ul></div></div></nav></header><div class="container-fluid"><div class="row min-vh-100"><nav id="course-sidebar" class="col-md-3 col-lg-2 d-md-block sidebar collapse"><div class="position-sticky pt-3"><ul class="nav flex-column course-topics-list"><a class="nav-link course-link active" href="/geekmonks/ai-ml/llm-engineering" data-course="ai-ml/llm-engineering"> AI/ML </a><ul class="nav flex-column ms-2 subtopics" id="topics-ai-ml-llm-engineering"><li class="nav-item" style="text-align: left"><a class="nav-link topic-link small " href="introduction-to-llms.html" data-topic="introduction-to-llms.html" data-course-path="/ai-ml/llm-engineering"> LLM Fundamentals </a></li><li class="nav-item" style="text-align: left"><a class="nav-link topic-link small active" href="ollama-local-setup.html" data-topic="ollama-local-setup.html" data-course-path="/ai-ml/llm-engineering"> Lab - Ollama Local LLMs </a></li><li class="nav-item" style="text-align: left"><a class="nav-link topic-link small " href="b1-t1-hello-world.html" data-topic="b1-t1-hello-world.html" data-course-path="/ai-ml/llm-engineering"> Lab - Python App Integration </a></li><li class="nav-item" style="text-align: left"><a class="nav-link topic-link small " href="ollama-ai-assistant.html" data-topic="ollama-ai-assistant.html" data-course-path="/ai-ml/llm-engineering"> Lab - Local AI Assistant </a></li><li class="nav-item" style="text-align: left"><a class="nav-link topic-link small " href="frontier-models-capabilities-operational-risks.html" data-topic="frontier-models-capabilities-operational-risks.html" data-course-path="/ai-ml/llm-engineering"> Architecture & capabilities </a></li><li class="nav-item" style="text-align: left"><a class="nav-link topic-link small " href="foundaion-and-evolution.html" data-topic="foundaion-and-evolution.html" data-course-path="/ai-ml/llm-engineering"> Foundation and Evaluation </a></li><li class="nav-item" style="text-align: left"><a class="nav-link topic-link small " href="advance-concepts-toknization-and-scaling.html" data-topic="advance-concepts-toknization-and-scaling.html" data-course-path="/ai-ml/llm-engineering"> Tokens, Scaling </a></li><li class="nav-item" style="text-align: left"><a class="nav-link topic-link small " href="scaling-reasoning-inference.html" data-topic="scaling-reasoning-inference.html" data-course-path="/ai-ml/llm-engineering"> Reasoning & Interference </a></li><li class="nav-item" style="text-align: left"><a class="nav-link topic-link small " href="payload-tokenization.html" data-topic="payload-tokenization.html" data-course-path="/ai-ml/llm-engineering"> Payload Tokenization </a></li><li class="nav-item" style="text-align: left"><a class="nav-link topic-link small " href="advance-conectps-models-deployments.html" data-topic="advance-conectps-models-deployments.html" data-course-path="/ai-ml/llm-engineering"> Reasoning Efforts </a></li><li class="nav-item" style="text-align: left"><a class="nav-link topic-link small " href="advance-prompt-caching.html" data-topic="advance-prompt-caching.html" data-course-path="/ai-ml/llm-engineering"> Prompt Caching </a></li></ul></ul></div></nav><main class="col-md-9 ms-sm-auto col-lg-10 px-md-4"><button id="sidebarToggle" class="btn btn-sm btn-outline-secondary d-md-none mb-2" aria-label="Toggle topics" aria-controls="course-sidebar"> ☰ Topics </button><div id="topic-content"><div id="pageTitle"><h1>Ollama - Run LLMs Locally</h1><p style="font-style: italic;"> Updated on: 07 Oct 2025 - <a href="/geekmonks/authors/srvivek/">Vivek Singh</a></p><hr class="stylish-hr"/></div><p>Let’s dive into setting up a robust local development environment suitable for begineer/intermediate LLM engineering tasks. We focus on modern, efficient tools like <strong>Ollama</strong> for running models locally and <strong>uv</strong> as an ultrafast python package manager.</p><h2 id="table-of-contents">Table of Contents</h2><ul><li><a href="#table-of-contents">Table of Contents</a></li><li><a href="#1-overview">1. Overview</a></li><li><a href="#2-ollama-installation-for-local-models">2. Ollama Installation for Local Models</a><ul><li><a href="#installation-instructions">Installation Instructions</a></li><li><a href="#post-installation-verification">Post-Installation Verification</a></li><li><a href="#api-verification">API verification</a></li></ul></li><li><a href="#3-integrated-development-environment-ide-setup">3. Integrated Development Environment (IDE) Setup</a><ul><li><a href="#ide-choice-and-update">IDE Choice and Update</a></li><li><a href="#required-extensions-vs-codecursor">Required Extensions (VS Code/Cursor)</a></li></ul></li><li><a href="#4-the-uv-python-package-and-project-manager">4. The <code class="language-plaintext highlighter-rouge">uv</code> Python Package and Project Manager</a><ul><li><a href="#installation-of-uv">Installation of <code class="language-plaintext highlighter-rouge">uv</code></a></li><li><a href="#verification-and-maintenance">Verification and Maintenance</a></li></ul></li><li><a href="#5-building-the-python-project">5. Building the Python Project</a><ul><li><a href="#project-synchronization">Project Synchronization</a></li></ul></li><li><a href="#6-troubleshooting-missing-jupyter-kernel">6. Troubleshooting: Missing Jupyter Kernel</a><ul><li><a href="#manual-kernel-registration-steps">Manual Kernel Registration Steps</a></li></ul></li><li><a href="#7-conclusion-and-key-takeaways">7. Conclusion and Key Takeaways</a><ul><li><a href="#key-takeaways">Key Takeaways</a></li></ul></li></ul><hr/><h2 id="1-overview">1. Overview</h2><p>A stable, performant local environment is critical for iterating quickly on LLM projects. This setup utilizes <strong>Ollama</strong> for running open-source models (like Llama 3 or Mistral) directly on your machine and <strong>VS Code/Cursor</strong> as the primary development platform, enhanced by the Rust-based package manager <strong>uv</strong> for superior speed and reliability.</p><hr/><h2 id="2-ollama-installation-for-local-models">2. Ollama Installation for Local Models</h2><p><strong>Ollama</strong> simplifies running and managing large language models locally. It provides a single executable for downloading, configuring, and serving models via an API.</p><h3 id="installation-instructions">Installation Instructions</h3><p>Follow the platform-specific instructions below. The primary goal is to install the <code class="language-plaintext highlighter-rouge">ollama</code> command-line tool.</p><ul><li><strong>Linux (Recommended via Script):</strong> The installation script automatically handles setting up the service. <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-fsSL</span> https://ollama.com/install.sh | sh
</code></pre></div></div></li><li><strong>macOS and Windows (GUI/Installer):</strong> For these platforms, use the dedicated installers which often include a graphical user interface (GUI) and better system integration. <ul><li><strong>macOS Installer:</strong><a href="https://ollama.com/download/mac">Download Ollama for Mac</a></li><li><strong>Windows Installer:</strong><a href="https://ollama.com/download/windows">Download Ollama for Windows</a></li></ul></li></ul><h3 id="post-installation-verification">Post-Installation Verification</h3><p>Once installed, you can pull and run your first model, for example, the tiny <code class="language-plaintext highlighter-rouge">llama3:8b</code> model:</p><div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ollama run llama3:8b
</code></pre></div></div><p>This command downloads the model if it’s not present and starts an interactive chat session via <strong><em>terminal/CMD</em></strong>.</p><h3 id="api-verification">API verification</h3><p>You can run below curl command from terminal or use any availale HTTP Client tool to mimic below request and send the message to Ollama API.</p><ul><li>If the provided LLM model is available locally, <code class="language-plaintext highlighter-rouge">Ollama</code> with run the model and servs the request. While running model in this way Ollama sets a default max-life of 4 Minutes for that model post which the model will be terminated automatically.</li><li><code class="language-plaintext highlighter-rouge">Stream</code> property is be default set to <code class="language-plaintext highlighter-rouge">True</code>. This enables the Ollama to keep sending the data to terminal as the model thinks and generates the response but if set to <code class="language-plaintext highlighter-rouge">False</code> it will process the request and send only the final response back to requestor.</li></ul><div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl http://localhost:11434/v1/chat/completions <span class="se">\</span>
  <span class="nt">-H</span> <span class="s2">"Content-Type: application/json"</span> <span class="se">\</span>
  <span class="nt">-d</span> <span class="s1">'{
    "model": "gemma3:1b",
    "messages": [{"role": "user", "content": "Tell me a fun fact"}],
    "stream": false
  }'</span>
</code></pre></div></div><hr/><h2 id="3-integrated-development-environment-ide-setup">3. Integrated Development Environment (IDE) Setup</h2><p>For LLM engineering, an IDE with robust support for <strong>Python</strong>, <strong>Jupyter Notebooks</strong>, and <strong>AI-assisted coding assistand</strong> is essential.</p><h3 id="ide-choice-and-update">IDE Choice and Update</h3><ul><li><strong>Recommended IDEs:</strong><strong>Visual Studio Code (VS Code)</strong> or <strong>Cursor</strong>. <ul><li>Cursor is often preferred for its built-in, deep AI coding agent capabilities, but VS Code is the industry standard.</li></ul></li><li><strong>Action:</strong> Ensure you are running the <strong>latest stable version</strong> of your chosen IDE for the best feature and security support.</li></ul><h3 id="required-extensions-vs-codecursor">Required Extensions (VS Code/Cursor)</h3><p>Two core extensions from Microsoft are necessary to support a modern Python and data science workflow:</p><ol><li><strong>Python Extension (Microsoft):</strong> Provides rich support for Python development, including IntelliSense, debugging, code navigation, and environment management. <ul><li><em>Tip:</em> Search the VS Code Marketplace for <code class="language-plaintext highlighter-rouge">ms-python.python</code>.</li></ul></li><li><strong>Jupyter Extension (Microsoft):</strong> Enables full functionality for working with <code class="language-plaintext highlighter-rouge">.ipynb</code> files, including variable inspection, cell execution, and kernel management, crucial for research and prototyping in LLM development. <ul><li><em>Tip:</em> Search the VS Code Marketplace for <code class="language-plaintext highlighter-rouge">ms-toolsai.jupyter</code>.</li></ul></li></ol><hr/><h2 id="4-the-uv-python-package-and-project-manager">4. The <code class="language-plaintext highlighter-rouge">uv</code> Python Package and Project Manager</h2><p><strong>uv</strong> is an extremely fast and reliable Python package and project manager written in Rust, designed to be a drop-in replacement for common <code class="language-plaintext highlighter-rouge">pip</code>, <code class="language-plaintext highlighter-rouge">pip-tools</code>, and <code class="language-plaintext highlighter-rouge">virtualenv</code> workflows. It significantly accelerates dependency resolution and virtual environment creation.</p><h3 id="installation-of-uv">Installation of <code class="language-plaintext highlighter-rouge">uv</code></h3><p>The recommended method is via the installation script:</p><div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install uv via the official script</span>
curl <span class="nt">-LsSf</span> https://astral.sh/uv/install.sh | sh
</code></pre></div></div><h3 id="verification-and-maintenance">Verification and Maintenance</h3><p>After installation, verify the version and know how to keep it updated:</p><ul><li><strong>Verify Installation:</strong><div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv <span class="nt">--version</span>
</code></pre></div></div></li><li><strong>Update <code class="language-plaintext highlighter-rouge">uv</code>:</strong> Keep your <code class="language-plaintext highlighter-rouge">uv</code> installation current to benefit from the latest speed improvements and features. <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv self update
</code></pre></div></div></li></ul><hr/><h2 id="5-building-the-python-project">5. Building the Python Project</h2><p>Assuming you have a standard Python project structure and a <code class="language-plaintext highlighter-rouge">requirements.txt</code> or <code class="language-plaintext highlighter-rouge">pyproject.toml</code> file defining your dependencies, <code class="language-plaintext highlighter-rouge">uv</code> simplifies the process of creating a virtual environment (<code class="language-plaintext highlighter-rouge">venv</code>) and installing dependencies.</p><h3 id="project-synchronization">Project Synchronization</h3><p>The <code class="language-plaintext highlighter-rouge">uv sync</code> command is the equivalent of <code class="language-plaintext highlighter-rouge">pip install -r requirements.txt</code> or <code class="language-plaintext highlighter-rouge">pip-sync</code>, but much faster. It creates the virtual environment and installs/updates all dependencies defined in your project files.</p><div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Command to build the virtual environment and install dependencies</span>
uv <span class="nb">sync</span>
</code></pre></div></div><ul><li><strong>Note:</strong> If a <code class="language-plaintext highlighter-rouge">.venv</code> directory does not exist, <code class="language-plaintext highlighter-rouge">uv</code> will automatically create it based on your project files and then synchronize the required packages inside it.</li></ul><hr/><h2 id="6-troubleshooting-missing-jupyter-kernel">6. Troubleshooting: Missing Jupyter Kernel</h2><p>A common issue in IDEs like VS Code is the inability to detect the dedicated kernel within a new virtual environment when opening a Jupyter Notebook. This prevents running code cells with the project’s specific dependencies.</p><p>To resolve this, manually register the virtual environment as a usable Jupyter kernel.</p><h3 id="manual-kernel-registration-steps">Manual Kernel Registration Steps</h3><ol><li><p><strong>Activate the Virtual Environment:</strong> Navigate to your project root and activate the environment to ensure <code class="language-plaintext highlighter-rouge">uv</code> and <code class="language-plaintext highlighter-rouge">python</code> are linked to the correct binaries.</p><div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">source</span> .venv/bin/activate
</code></pre></div></div></li><li><p><strong>Run the <code class="language-plaintext highlighter-rouge">ipykernel</code> Install Command:</strong> Use <code class="language-plaintext highlighter-rouge">uv run</code> to execute the Python interpreter within the virtual environment, calling the <code class="language-plaintext highlighter-rouge">ipykernel</code> module to register the environment.</p><ul><li><code class="language-plaintext highlighter-rouge">--user</code>: Installs the kernel for the current user.</li><li><code class="language-plaintext highlighter-rouge">--name</code>: The internal name for the kernel (e.g., <code class="language-plaintext highlighter-rouge">llm-eng</code>).</li><li><code class="language-plaintext highlighter-rouge">--display-name</code>: The name visible in the IDE’s kernel selector.</li></ul><div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv run python <span class="nt">-m</span> ipykernel <span class="nb">install</span> <span class="nt">--user</span> <span class="nt">--name</span><span class="o">=</span>llm-eng <span class="nt">--display-name</span><span class="o">=</span><span class="s2">"LLM Eng (.venv)"</span>
</code></pre></div></div></li></ol><p>After running this, the kernel named <strong>“LLM Eng (.venv)”</strong> will appear in the kernel selector of your Jupyter Notebook in VS-Code, allowing you to execute the code blocks in <code class="language-plaintext highlighter-rouge">.ipynb</code> files directly from IDE with your project’s installed packages.</p><hr/><h2 id="7-conclusion-and-key-takeaways">7. Conclusion and Key Takeaways</h2><p>The modern LLM engineering workflow prioritizes speed and local experimentation. By leveraging <strong>Ollama</strong> for model serving and <strong>uv</strong> for dependency management, developers can achieve rapid iteration cycles.</p><h3 id="key-takeaways">Key Takeaways</h3><ul><li><strong>Ollama:</strong> Enables simple, powerful execution of open-source LLMs locally, reducing cloud costs during development.</li><li><strong>uv:</strong> Drastically cuts down the time spent on dependency resolution and environment setup, freeing up time for actual engineering.</li><li><strong>IDE Setup:</strong> Installing the <strong>Python</strong> and <strong>Jupyter</strong> extensions is non-negotiable for a professional LLM development experience.</li><li><strong>Kernel Registration:</strong> Knowing how to manually register a kernel is an essential troubleshooting skill for any developer working with isolated Python environments and Jupyter notebooks.</li></ul><p>Would you like to move on to documenting the LLM-specific techniques, such as <strong>Prompt Engineering</strong> and <strong>RAG vs. Fine-Tuning</strong>, that were mentioned in the session overview?</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code></code></pre></div></div></div></main></div></div><div><footer class="row row-cols-1 row-cols-sm-2 row-cols-md-5 py-4 mx-5 my-5 border-top"><div class="col-md-4 d-flex align-items-center"><a href="/geekmonks/" class="mb-3 me-2 mb-md-0 text-body-secondary text-decoration-none lh-1" aria-label="Geekmonks"><svg class="bi me-2" width="40" height="32" aria-hidden="true" viewBox="0 0 200 200"><defs><linearGradient id="gradient" gradientTransform="rotate(145 0.5 0.5)"><stop offset="0%" stop-color="#f7e625"></stop><stop offset="100%" stop-color="#e6a62c"></stop></linearGradient></defs><rect width="200" height="200" fill="url('#gradient')"></rect><g fill="#303e37" transform="matrix(12.312625250501002,0,0,12.312625250501002,16.825404661093543,187.54701945968048)" stroke="#3b8349" stroke-width="0.2"><path d="M12.71-7.60L12.71-1.76Q11.92-0.88 10.40-0.34Q8.88 0.20 7.06 0.20L7.06 0.20Q4.27 0.20 2.60-1.51Q0.93-3.22 0.81-6.27L0.81-6.27L0.80-7.50Q0.80-9.60 1.54-11.17Q2.29-12.73 3.67-13.58Q5.05-14.42 6.87-14.42L6.87-14.42Q9.52-14.42 10.99-13.21Q12.46-11.99 12.71-9.58L12.71-9.58L9.40-9.58Q9.23-10.77 8.64-11.28Q8.06-11.79 6.98-11.79L6.98-11.79Q5.69-11.79 4.99-10.69Q4.29-9.60 4.28-7.57L4.28-7.57L4.28-6.71Q4.28-4.58 5.00-3.51Q5.73-2.44 7.29-2.44L7.29-2.44Q8.63-2.44 9.29-3.04L9.29-3.04L9.29-5.24L6.90-5.24L6.90-7.60L12.71-7.60Z"></path></g></svg></a><span class="mb-3 mb-md-0 text-body-secondary">© 2025 Company, Inc</span></div><div class="col mb-3"></div><div class="col mb-3"><h5>Company</h5><ul class="nav col-md-4"><li class="nav-item"><a href="/geekmonks/" class="nav-link px-2 text-body-secondary">Home</a></li><li class="nav-item"><a href="/geekmonks/about" class="nav-link px-2 text-body-secondary">About</a></li><li class="nav-item"><a href="https://github.com/SRVivek1/" target="_blank" class="nav-link px-2 text-body-secondary">Github</a></li><li class="nav-item"><a href="https://www.linkedin.com/in/srvivek1/" target="_blank" class="nav-link px-2 text-body-secondary">LinkedIn</a></li></ul></div><div class="col mb-3"><h5>Tutorials</h5><ul class="nav flex-column"><li class="nav-item mb-2"><a href="/geekmonks/spring/spring-boot" class="nav-link p-0 text-body-secondary">Spring Boot</a></li><li class="nav-item mb-2"><a href="/geekmonks/ai-ml/llm-engineering" class="nav-link p-0 text-body-secondary">AI / ML</a></li><li class="nav-item mb-2"><a href="/geekmonks/cloud/aws" class="nav-link p-0 text-body-secondary">AWS</a></li></ul></div></footer></div><script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script><script src="/geekmonks/assets/js/mermaid-diagram-render.js?v=0.1.0"></script><script src="/geekmonks/assets/js/submenu-togle.js?v=0.1.0"></script></body></html>