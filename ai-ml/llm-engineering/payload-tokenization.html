<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet"><link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css" rel="stylesheet"><link rel="stylesheet" href="/geekmonks/assets/css/styles.css"><link type="application/atom+xml" rel="alternate" href="https://srvivek1.github.io/geekmonks/feed.xml" title="Geekmonks Blog"/><title>üí° Tokenization - Core Concepts &amp; Best Practices | Geekmonks Blog</title><meta name="generator" content="Jekyll v3.10.0"/><meta property="og:title" content="üí° Tokenization - Core Concepts &amp; Best Practices"/><meta property="og:locale" content="en_US"/><meta name="description" content="Comprehensive notes on LLM Engineering, covering Prompt Engineering, RAG vs. Fine-Tuning, Evaluation Metrics (Perplexity, BLEU), and the fundamental role of Tokens. Optimized for developers new to advanced LLM topics."/><meta property="og:description" content="Comprehensive notes on LLM Engineering, covering Prompt Engineering, RAG vs. Fine-Tuning, Evaluation Metrics (Perplexity, BLEU), and the fundamental role of Tokens. Optimized for developers new to advanced LLM topics."/><link rel="canonical" href="https://srvivek1.github.io/geekmonks/ai-ml/llm-engineering/payload-tokenization.html"/><meta property="og:url" content="https://srvivek1.github.io/geekmonks/ai-ml/llm-engineering/payload-tokenization.html"/><meta property="og:site_name" content="Geekmonks Blog"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2025-11-01T00:00:00+05:30"/><meta name="twitter:card" content="summary"/><meta property="twitter:title" content="üí° Tokenization - Core Concepts &amp; Best Practices"/><script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-11-01T00:00:00+05:30","datePublished":"2025-11-01T00:00:00+05:30","description":"Comprehensive notes on LLM Engineering, covering Prompt Engineering, RAG vs. Fine-Tuning, Evaluation Metrics (Perplexity, BLEU), and the fundamental role of Tokens. Optimized for developers new to advanced LLM topics.","headline":"üí° Tokenization - Core Concepts &amp; Best Practices","mainEntityOfPage":{"@type":"WebPage","@id":"https://srvivek1.github.io/geekmonks/ai-ml/llm-engineering/payload-tokenization.html"},"url":"https://srvivek1.github.io/geekmonks/ai-ml/llm-engineering/payload-tokenization.html"}</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-48XEHMVSEC"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date);gtag("config","G-48XEHMVSEC");</script></head><body class="container-fluid p-0 m-0"><header><nav class="navbar navbar-expand-md navbar-light bg-light text-white content-shadow"><div class="container d-flex justify-content-between"><a href="/geekmonks/" class="navbar-brand d-flex"><img class="d-inline-block align-top" src="/geekmonks/assets/icons/laptop.svg" height="40" width="34"/><h1 class="fs-3 px-2 fw-bold">Geekmonks</h1><img class="d-inline-block align-top" src="/geekmonks/assets/icons/tux.svg" height="40" width="34"/></a><button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNavBanner" aria-controls="navbarNavBanner" aria-expanded="false" aria-label="Toggle Navigation"><span class="navbar-toggler-icon"></span></button><div id="navbarNavBanner" class="collapse navbar-collapse"><ul class="navbar-nav ms-auto"><li class="nav-item active"><a href="/geekmonks/" class="nav-link fw-bold active">Home</a></li><li class="nav-item active"><a href="/geekmonks/about" class="nav-link fw-bold active">About</a></li><li class="nav-item active"><a href="https://github.com/SRVivek1/" class="nav-link fw-bold active" target="_blank" rel="noopener noreferrer">Github</a></li><li class="nav-item active"><a href="https://www.linkedin.com/in/srvivek1/" class="nav-link fw-bold active" target="_blank" rel="noopener noreferrer">LinkedIn</a></li></ul></div></div></nav><nav class="navbar navbar-expand-xl navbar-custom p-0"><div class="container-fluid"><button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNav"><ul class="navbar-nav mx-auto"><li class="nav-item"><a class="nav-link fw-bold text-white" href="/geekmonks/ai-ml/llm-engineering/">AI-ML</a></li><li class="nav-item"><a class="nav-link fw-bold text-white" href="/geekmonks/cloud/aws/">AWS</a></li><li class="nav-item"><a class="nav-link fw-bold text-white" href="/geekmonks/spring/spring-boot/">Spring Boot</a></li></ul></div></div></nav></header><div class="container-fluid"><div class="row min-vh-100"><nav id="course-sidebar" class="col-md-3 col-lg-2 d-md-block bg-light sidebar collapse"><div class="position-sticky pt-3"><ul class="nav flex-column course-topics-list"><a class="nav-link course-link active" href="/geekmonks/ai-ml/llm-engineering" data-course="ai-ml/llm-engineering"> AI/ML </a><ul class="nav flex-column ms-3 subtopics" id="topics-ai-ml-llm-engineering"><li class="nav-item" style="text-align: left"><a class="nav-link topic-link small " href="introduction-to-llms.html" data-topic="introduction-to-llms.html" data-course-path="/ai-ml/llm-engineering"> LLM Fundamentals </a></li><li class="nav-item" style="text-align: left"><a class="nav-link topic-link small " href="ollama-local-setup.html" data-topic="ollama-local-setup.html" data-course-path="/ai-ml/llm-engineering"> Lab - Ollama Local LLMs </a></li><li class="nav-item" style="text-align: left"><a class="nav-link topic-link small " href="b1-t1-hello-world.html" data-topic="b1-t1-hello-world.html" data-course-path="/ai-ml/llm-engineering"> Lab - Python App Integration </a></li><li class="nav-item" style="text-align: left"><a class="nav-link topic-link small " href="ollama-ai-assistant.html" data-topic="ollama-ai-assistant.html" data-course-path="/ai-ml/llm-engineering"> Lab - Local AI Assistant </a></li><li class="nav-item" style="text-align: left"><a class="nav-link topic-link small " href="frontier-models-capabilities-operational-risks.html" data-topic="frontier-models-capabilities-operational-risks.html" data-course-path="/ai-ml/llm-engineering"> Architecture & capabilities </a></li><li class="nav-item" style="text-align: left"><a class="nav-link topic-link small " href="foundaion-and-evolution.html" data-topic="foundaion-and-evolution.html" data-course-path="/ai-ml/llm-engineering"> Foundation and Evaluation </a></li><li class="nav-item" style="text-align: left"><a class="nav-link topic-link small " href="advance-concepts-toknization-and-scaling.html" data-topic="advance-concepts-toknization-and-scaling.html" data-course-path="/ai-ml/llm-engineering"> Advanced - Tokens, Scaling </a></li><li class="nav-item" style="text-align: left"><a class="nav-link topic-link small " href="scaling-reasoning-inference.html" data-topic="scaling-reasoning-inference.html" data-course-path="/ai-ml/llm-engineering"> Advanced - Reasoning, Interference </a></li><li class="nav-item" style="text-align: left"><a class="nav-link topic-link small active" href="payload-tokenization.html" data-topic="payload-tokenization.html" data-course-path="/ai-ml/llm-engineering"> Advanced - Payload Tokenization </a></li><li class="nav-item" style="text-align: left"><a class="nav-link topic-link small " href="advance-conectps-models-deployments.html" data-topic="advance-conectps-models-deployments.html" data-course-path="/ai-ml/llm-engineering"> Advanced Concepts & Deployment </a></li><li class="nav-item" style="text-align: left"><a class="nav-link topic-link small " href="advance-prompt-caching.html" data-topic="advance-prompt-caching.html" data-course-path="/ai-ml/llm-engineering"> Advanced - Prompt Caching </a></li></ul></ul></div></nav><main class="col-md-9 ms-sm-auto col-lg-10 px-md-4"><button id="sidebarToggle" class="btn btn-sm btn-outline-secondary d-md-none mb-2" aria-label="Toggle topics" aria-controls="course-sidebar"> ‚ò∞ Topics </button><div id="topic-content"><div id="pageTitle"><h1>üí° Tokenization - Core Concepts & Best Practices</h1><p> Updated on: 01 Nov 2025 - <a href="/geekmonks/authors/srvivek/">Vivek Singh</a></p><hr class="stylish-hr"/></div><h2 id="table-of-contents">Table of Contents</h2><ul><li><a href="#table-of-contents">Table of Contents</a></li><li><a href="#1-introduction">1. Introduction</a></li><li><a href="#2-the-foundation-understanding-tokens">2. The Foundation: Understanding Tokens</a><ul><li><a href="#21-tokens-the-middle-ground-for-language-models">2.1. Tokens: The Middle Ground for Language Models</a></li></ul></li><li><a href="#3-key-llm-engineering-techniques">3. Key LLM Engineering Techniques</a><ul><li><a href="#31-prompt-engineering-guiding-the-llm">3.1. Prompt Engineering: Guiding the LLM</a></li></ul></li><li><a href="#4-llm-customization-strategies">4. LLM Customization Strategies</a><ul><li><a href="#41-fine-tuning-vs-retrieval-augmented-generation-rag">4.1. Fine-Tuning vs. Retrieval-Augmented Generation (RAG)</a></li></ul></li><li><a href="#5-evaluation-and-measurement">5. Evaluation and Measurement</a><ul><li><a href="#51-evaluation-metrics-for-llms">5.1. Evaluation Metrics for LLMs</a></li></ul></li><li><a href="#6-essential-tools-in-the-llm-ecosystem">6. Essential Tools in the LLM Ecosystem</a></li><li><a href="#7-major-challenges-in-llm-deployment">7. Major Challenges in LLM Deployment</a></li><li><a href="#8-key-takeaways-and-conclusion">8. Key Takeaways and Conclusion</a></li></ul><hr/><h2 id="1-introduction">1. Introduction</h2><p>LLM Engineering is the discipline of effectively building and optimizing applications using Large Language Models (LLMs). It moves beyond basic model training to focus on <strong>prompt design, context injection, integration, and reliable deployment</strong>. Mastering this field is crucial for turning powerful foundation models into reliable, production-ready systems.</p><hr/><h2 id="2-the-foundation-understanding-tokens">2. The Foundation: Understanding Tokens</h2><p>The fundamental unit of data an LLM processes is the <strong>token</strong>. Understanding how tokens work is essential for managing model context length, controlling costs, and designing effective prompts.</p><h3 id="21-tokens-the-middle-ground-for-language-models">2.1. Tokens: The Middle Ground for Language Models</h3><p>The use of tokens represents an evolution in how language models process text, offering a balance between vocabulary size and model efficiency.</p><table><thead><tr><th style="text-align: left">Historical Method</th><th style="text-align: left">Description</th><th style="text-align: left">Pros</th><th style="text-align: left">Cons</th></tr></thead><tbody><tr><td style="text-align: left"><strong>Character-by-Character</strong></td><td style="text-align: left">Model predicts the next <em>letter</em>.</td><td style="text-align: left">Small, fixed vocabulary (letters, punctuation).</td><td style="text-align: left">Expects the network to learn both syntax and semantics from a simple alphabet. <strong>Too much abstraction is required</strong>.</td></tr><tr><td style="text-align: left"><strong>Word-by-Word</strong></td><td style="text-align: left">Model predicts the next <em>full word</em>.</td><td style="text-align: left">Much easier for the model to learn context.</td><td style="text-align: left"><strong>Enormous vocabulary</strong> (millions of words), leading to sparse data and difficulty handling rare/new words (Out-of-Vocabulary or OOV).</td></tr><tr><td style="text-align: left"><strong>Token (Subword) Based</strong></td><td style="text-align: left">Model predicts the next <em>subword unit</em> (token).</td><td style="text-align: left"><strong>Managable vocabulary</strong> size. Elegantly handles word stems, conjugations, and new/compound words by breaking them into known parts.</td><td style="text-align: left">Can be less intuitive to count than words.</td></tr></tbody></table><ul><li><strong>What is a Token?</strong> A token is a <em>chunk of characters</em> that can be a full common word (<code class="language-plaintext highlighter-rouge">hello</code>), a sub-word unit (<code class="language-plaintext highlighter-rouge">runn</code>, <code class="language-plaintext highlighter-rouge">ing</code>), or punctuation (<code class="language-plaintext highlighter-rouge">.</code>, <code class="language-plaintext highlighter-rouge">!</code>).</li><li><strong>The Breakthrough:</strong> Subword tokenization (e.g., Byte-Pair Encoding or BPE) provides the <strong>optimal middle ground</strong>. A word can be broken into multiple parts, and each part is treated as a separate token. For example, the word <strong>‚Äútokenization‚Äù</strong> might be broken into the tokens: <code class="language-plaintext highlighter-rouge">token</code>, <code class="language-plaintext highlighter-rouge">iz</code>, <code class="language-plaintext highlighter-rouge">ation</code>.</li><li><strong>Practical Example:</strong> You can see the impact of this on the <a href="https://platform.openai.com/tokenizer">OpenAI Tokenizer</a>. Inputting a complex or hyphenated word often shows it being split into 2-3 tokens, while a simple common word like ‚ÄúThe‚Äù is often a single token.</li><li><strong>Rule of Thumb for Token Generation:</strong> A rough estimation is that <strong>1,000 tokens</strong> often equate to <strong>$\approx 750$ words</strong> in English. This ratio is crucial for estimating prompt/response cost and managing the model‚Äôs <strong>context window</strong> (the maximum number of tokens an LLM can process at once).</li></ul><hr/><h2 id="3-key-llm-engineering-techniques">3. Key LLM Engineering Techniques</h2><h3 id="31-prompt-engineering-guiding-the-llm">3.1. Prompt Engineering: Guiding the LLM</h3><p><strong>Prompt Engineering</strong> is the art and science of designing inputs (prompts) that steer the LLM toward a desired, high-quality, and reliable output. It minimizes the need for costly fine-tuning by maximizing the model‚Äôs inherent reasoning capabilities.</p><table><thead><tr><th style="text-align: left">Technique</th><th style="text-align: left">Description</th><th style="text-align: left">Simple Example</th></tr></thead><tbody><tr><td style="text-align: left"><strong>Chain-of-Thought (CoT)</strong></td><td style="text-align: left">Instructs the model to <strong>show its reasoning steps</strong> before giving the final answer. This dramatically improves accuracy on complex arithmetic or logic problems.</td><td style="text-align: left"><strong>Prompt:</strong> ‚ÄúLet‚Äôs think step by step. If a box has 5 red and 3 blue balls, and I remove 2 red balls, how many balls are left?‚Äù</td></tr><tr><td style="text-align: left"><strong>Few-Shot Prompting</strong></td><td style="text-align: left">Providing the model with <strong>2-5 examples</strong> of an input-output pair before asking the target question. This helps the model infer the desired format, style, or task.</td><td style="text-align: left"><strong>Prompt:</strong><em>Example 1 (Input/Output), Example 2 (Input/Output),</em> ‚ÄúNow classify the following text: [New Text]‚Äù</td></tr></tbody></table><hr/><h2 id="4-llm-customization-strategies">4. LLM Customization Strategies</h2><p>When a base LLM is not sufficient, there are two primary methods for injecting domain-specific knowledge: <strong>Fine-Tuning</strong> and <strong>Retrieval-Augmented Generation (RAG)</strong>.</p><h3 id="41-fine-tuning-vs-retrieval-augmented-generation-rag">4.1. Fine-Tuning vs. Retrieval-Augmented Generation (RAG)</h3><table><thead><tr><th style="text-align: left">Feature</th><th style="text-align: left">Fine-Tuning</th><th style="text-align: left">Retrieval-Augmented Generation (RAG)</th></tr></thead><tbody><tr><td style="text-align: left"><strong>Goal</strong></td><td style="text-align: left"><strong>Modify the model‚Äôs weights</strong> to teach it a new style, format, or dense facts (e.g., proprietary code standards).</td><td style="text-align: left"><strong>Augment the prompt</strong> with relevant external data (retrieved via vector search) to ground its response.</td></tr><tr><td style="text-align: left"><strong>Knowledge Source</strong></td><td style="text-align: left"><strong>Learned</strong> and permanently stored in the model‚Äôs parameters (weights).</td><td style="text-align: left"><strong>External</strong> and dynamically retrieved from a database (e.g., a Vector Store) for each query.</td></tr><tr><td style="text-align: left"><strong>Cost &amp; Time</strong></td><td style="text-align: left"><strong>High</strong> (requires large, labeled dataset, significant GPU time).</td><td style="text-align: left"><strong>Low</strong> (requires building a search index, fast lookup at inference time).</td></tr><tr><td style="text-align: left"><strong>Update Process</strong></td><td style="text-align: left"><strong>Slow</strong> (requires re-running the entire fine-tuning process).</td><td style="text-align: left"><strong>Fast</strong> (just update the external database/index).</td></tr><tr><td style="text-align: left"><strong>Hallucination Risk</strong></td><td style="text-align: left"><strong>Moderate-High</strong> (if trained on bad data or asked about facts outside its new knowledge).</td><td style="text-align: left"><strong>Low</strong> (answers are <em>grounded</em> in the retrieved source text, which can be cited).</td></tr><tr><td style="text-align: left"><strong>Best For</strong></td><td style="text-align: left">New <strong>tasks</strong>, <strong>style</strong>, <strong>persona</strong>, or complex <strong>reasoning patterns</strong>.</td><td style="text-align: left"><strong>Up-to-date facts</strong>, proprietary <strong>documents</strong>, or <strong>long-tail knowledge</strong> that changes frequently.</td></tr></tbody></table><p><strong>RAG Architecture:</strong> The RAG process involves <strong>three steps</strong>:</p><ul><li><strong>1.</strong> A user query is converted into an embedding.</li><li><strong>2.</strong> This embedding is used to search a Vector Database, retrieving the most <em>semantically similar</em> chunks of text (context).</li><li><strong>3.</strong> The original query and the retrieved context are combined into a final prompt, which is sent to the LLM.</li></ul><hr/><h2 id="5-evaluation-and-measurement">5. Evaluation and Measurement</h2><p>Reliable deployment requires robust evaluation. LLMs, especially in generative tasks, are challenging to grade using simple metrics.</p><h3 id="51-evaluation-metrics-for-llms">5.1. Evaluation Metrics for LLMs</h3><table><thead><tr><th style="text-align: left">Metric</th><th style="text-align: left">Definition</th><th style="text-align: left">Task/Use Case</th><th style="text-align: left">Challenge</th></tr></thead><tbody><tr><td style="text-align: left"><strong>Perplexity (PPL)</strong></td><td style="text-align: left">A measure of how well a probability distribution (the LLM) predicts a sample. Lower is better, indicating the model is <strong>less ‚Äúperplexed‚Äù</strong> by the text.</td><td style="text-align: left"><strong>Model Quality/Fluency.</strong> Often used to compare pre-trained models.</td><td style="text-align: left">Doesn‚Äôt measure factual accuracy or instruction following, only statistical fluency.</td></tr><tr><td style="text-align: left"><strong>BLEU (Bilingual Evaluation Understudy)</strong></td><td style="text-align: left">A classic metric measuring the <strong>n-gram overlap</strong> between a generated text and one or more reference texts. Scored 0 to 1 (or 0 to 100).</td><td style="text-align: left"><strong>Machine Translation, Summarization.</strong> Requires a human-written ‚Äòground truth‚Äô reference.</td><td style="text-align: left">Poorly correlates with human judgment for highly creative or diverse generations. Favors verbatim matches.</td></tr><tr><td style="text-align: left"><strong>ROUGE (Recall-Oriented Understudy for Gisting Evaluation)</strong></td><td style="text-align: left">Focuses on <strong>recall</strong>‚Äîhow many n-grams in the reference appear in the generated text.</td><td style="text-align: left"><strong>Summarization.</strong> Better than BLEU for capturing the key concepts of the source.</td><td style="text-align: left">Like BLEU, it relies on strict word/n-gram overlap and can miss semantic similarity.</td></tr><tr><td style="text-align: left"><strong>Human Evaluation</strong></td><td style="text-align: left">Subjective scoring by human reviewers based on <strong>Coherence, Fluency, and Faithfulness</strong> (factual accuracy).</td><td style="text-align: left"><strong>All tasks.</strong> The gold standard.</td><td style="text-align: left"><strong>Expensive</strong> and <strong>slow</strong> to scale. Reviewers can be inconsistent.</td></tr></tbody></table><hr/><h2 id="6-essential-tools-in-the-llm-ecosystem">6. Essential Tools in the LLM Ecosystem</h2><p>The LLM stack is rapidly evolving, but two platforms are foundational for any LLM engineer.</p><ul><li><strong>LangChain:</strong> A framework designed to simplify the creation of applications using LLMs. It provides <strong>modular components</strong> for chaining together prompts, LLMs, data sources (RAG), and agents. Key components include: <ul><li><strong>Chains:</strong> Sequences of calls (e.g., prompt $\rightarrow$ LLM $\rightarrow$ output parser).</li><li><strong>Agents:</strong> LLMs that use tools to interact with the world (e.g., search or code execution).</li><li><strong>Retrievers:</strong> Systems to pull relevant documents for RAG.</li></ul></li><li><strong>HuggingFace:</strong> The central hub for open-source AI. It hosts: <ul><li>The <strong>HuggingFace Hub:</strong> Thousands of pre-trained models (LLMs, vision, etc.), datasets, and demos.</li><li><strong><code class="language-plaintext highlighter-rouge">transformers</code> library:</strong> The standard library for downloading, training, and running state-of-the-art models.</li><li><strong>Accelerate:</strong> Tools for efficient multi-GPU and distributed training and fine-tuning.</li></ul></li></ul><hr/><h2 id="7-major-challenges-in-llm-deployment">7. Major Challenges in LLM Deployment</h2><p>Deploying LLMs reliably requires addressing known failure modes that impact user trust and application safety.</p><ul><li><strong>Hallucination:</strong> The LLM confidently generates content that is factually incorrect, nonsensical, or not supported by its training data or context. <ul><li><strong>Mitigation:</strong> Employing <strong>RAG</strong> (to ground answers in verifiable documents) and utilizing <strong>CoT</strong> (to check the model‚Äôs reasoning).</li></ul></li><li><strong>Bias:</strong> The LLM‚Äôs outputs reflect harmful stereotypes, prejudices, or unfair generalizations present in its massive training dataset. This can manifest in sensitive areas like hiring or finance. <ul><li><strong>Mitigation:</strong> Careful <strong>data curation</strong> during fine-tuning, implementing <strong>safety filters</strong> post-generation, and <strong>adversarial testing</strong> to identify bias vectors.</li></ul></li><li><strong>Context Window Limits:</strong> The maximum number of tokens an LLM can process is finite. Passing too much text results in truncation or error. <ul><li><strong>Mitigation:</strong> Intelligent <strong>document summarization</strong> before RAG, and effective <strong>chunking</strong> and retrieval to pass only the <em>most relevant</em> context.</li></ul></li></ul><hr/><h2 id="8-key-takeaways-and-conclusion">8. Key Takeaways and Conclusion</h2><p><strong>LLM Engineering is fundamentally about control and reliability.</strong></p><ul><li><strong>Tokens are your currency:</strong> Master token economics to manage costs and context effectively.</li><li><strong>Prompting is the quickest win:</strong> Utilize techniques like <strong>Chain-of-Thought</strong> and <strong>Few-Shot</strong> prompting to unlock the model‚Äôs full potential without retraining.</li><li><strong>RAG is essential for knowledge:</strong> For domain-specific or constantly changing facts, RAG offers a scalable, transparent, and updatable solution over costly fine-tuning.</li><li><strong>Evaluate rigorously:</strong> Move beyond simple metrics like Perplexity; implement human or LLM-as-a-Judge evaluations for critical tasks.</li></ul><p>Understanding these concepts and applying them with tools like <strong>LangChain</strong> and the <strong>HuggingFace</strong> ecosystem will enable you to build robust, trustworthy LLM applications.</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code></code></pre></div></div></div></main></div></div><script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script><script src="/geekmonks/assets/js/mermaid-diagram-render.js"></script><script src="/geekmonks/assets/js/submenu-togle.js"></script></body></html>